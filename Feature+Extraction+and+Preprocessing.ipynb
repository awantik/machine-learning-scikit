{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "one_hot_encoder = DictVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances = [ {'city':'Bangalore'}, {'city':'Tezpur'}, {'city':'Chennai'}, {'city':'Mumbai'}, {'city':'Allahabad'}, {'city':'Mangalore'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = one_hot_encoder.fit_transform(instances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# It creates coloumn for each possible options\n",
    "# Allahabad -> 1, Bangalore -> 2, Chennai -> 3, Mangalore -> 4, Mumbai -> 5 Tezpur -> 6, \n",
    "print encoder.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words\n",
    "\n",
    "This representation uses a multiset, or bag, that encodes the words that appear in a text; the bag-of-words does not encode any of the text's syntax, ignores the order of words, and disregards all grammar.\n",
    "One important understanding here is - text with similar words will have similar meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "'UNC played Duke in basketball',\n",
    "'Duke lost the basketball game',\n",
    "'I ate a sandwich'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "counts = vectorizer.fit_transform(corpus).todense()\n",
    "print counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'duke': 2, u'basketball': 1, u'lost': 5, u'played': 6, u'in': 4, u'game': 3, u'sandwich': 7, u'unc': 9, u'ate': 0, u'the': 8}\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "# Lesser eucliden_distance mean they are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.44948974]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distances(counts[0], counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.64575131]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distances(counts[1], counts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.64575131]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distances(counts[0], counts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 0 1]\n",
      " [0 1 1 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'duke': 2, u'basketball': 1, u'lost': 4, u'played': 5, u'game': 3, u'sandwich': 6, u'unc': 7, u'ate': 0}\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems with this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "{u'sandwich': 2, u'ate': 0, u'sandwiches': 3, u'eaten': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "'He ate the sandwiches',\n",
    "'Every sandwich was eaten by him'\n",
    "]\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "print vectorizer.fit_transform(corpus).todense()\n",
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print lemmatizer.lemmatize('gathering', 'v')\n",
    "print lemmatizer.lemmatize('gathering', 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print stemmer.stem('gathering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize our test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "wordnet_tags = ['n', 'v']\n",
    "corpus = [\n",
    "'He ate the sandwiches',\n",
    "'Everydays sandwich was eaten by him gooder nicee nicer'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed: [[u'He', u'ate', u'the', u'sandwich'], [u'Everyday', u'sandwich', u'wa', u'eaten', u'by', u'him', u'gooder', u'nice', u'nicer']]\n"
     ]
    }
   ],
   "source": [
    "# Stemming the corpus\n",
    "stemmer = PorterStemmer()\n",
    "print 'Stemmed:', [[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lemmatize(token, tag):\n",
    "    if tag[0].lower() in ['n', 'v']:\n",
    "        return lemmatizer.lemmatize(token, tag[0].lower())\n",
    "    return token\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pos_tag in module nltk.tag:\n",
      "\n",
      "pos_tag(tokens, tagset=None)\n",
      "    Use NLTK's currently recommended part of speech tagger to\n",
      "    tag the given list of tokens.\n",
      "    \n",
      "        >>> from nltk.tag import pos_tag\n",
      "        >>> from nltk.tokenize import word_tokenize\n",
      "        >>> pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"))\n",
      "        [('John', 'NNP'), (\"'s\", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is', 'VBZ'),\n",
      "        (\"n't\", 'RB'), ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'), ('.', '.')]\n",
      "        >>> pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"), tagset='universal')\n",
      "        [('John', 'NOUN'), (\"'s\", 'PRT'), ('big', 'ADJ'), ('idea', 'NOUN'), ('is', 'VERB'),\n",
      "        (\"n't\", 'ADV'), ('all', 'DET'), ('that', 'DET'), ('bad', 'ADJ'), ('.', '.')]\n",
      "    \n",
      "    NB. Use `pos_tag_sents()` for efficient tagging of more than one sentence.\n",
      "    \n",
      "    :param tokens: Sequence of tokens to be tagged\n",
      "    :type tokens: list(str)\n",
      "    :param tagset: the tagset to be used, e.g. universal, wsj, brown\n",
      "    :type tagset: str\n",
      "    :return: The tagged tokens\n",
      "    :rtype: list(tuple(str, str))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[\"this's a sent tokenize test.\", 'this is sent two.', 'is this sent three?', 'sent 4 is cool!', \"Now it's your turn.\"]\n"
     ]
    }
   ],
   "source": [
    "#Sentence Toekenizer\n",
    "text = \"this's a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it's your turn.\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "print len(sent_tokenize_list)\n",
    "print sent_tokenize_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '.']\n",
      "['this', \"'s\", 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "print word_tokenize('Hello World.')\n",
    "print word_tokenize(\"this's a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = word_tokenize(\"Dive into NLTK: Part-of-speech tagging and POS Tagger\")\n",
    "import nltk\n",
    "print nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gl-env]",
   "language": "python",
   "name": "conda-env-gl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
